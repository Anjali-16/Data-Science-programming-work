# Data-Science-programming-work

This repository contains information about the mini-projects, in-class exercises, and weekly exercises that I have done in my data science programming course.

It contains the projects that I solved with supervised machine learning algorithms, neural networks, and auto-encoders.

Each folder contains a particular algorithm and its application to a particular data set.

Assignment Business problem statement: We know how dreadful cancer is. It involves various stages of treatment. It is really important to analyze the results of treatment after the patient undergoes it. There are various factors that influence the treatment, such as age, gender, time, and number of warts. These are my predictors, and my target variable is the result of treatment. I apply all the algorithms discussed in this course and evaluate the model.

Walk through the Assignment 1 and 2 cryptography analysis folders to see my work and my model performance results. The data set was collected from the UCI machine learning repository. The data set is about the results of treatment for 90 patients. It is included in the folder.

Week 1: Introduction to Data Science and Machine Learning

I have learned the basics of data science, basic terms, and the end-to-end process of machine learning. I got introduced to the Jupyter Lab environment and learned the basics of Jupyter Lab markdown and the syntax of printing images in markdown. I worked on the data cleaning part, preprocessing the data, and making the data ready for modeling. I have understood the concepts of one-hot encoding and dummy encoding and applied them to the AirBnB data set.

Week 2: Regression:

I was introduced to regression techniques. I became strong in the concepts of simple linear regression, multivariable linear regression, polynomial regression, and logistic regression. I applied to the AirBnB data set. I observed how each regression performed on the data set. I also experimented by creating a DTAA set, adding some noise, and experiencing it. I also got introduced to the topics of regularization techniques called Ridge regression, Lasso regression, early stopping, elastic net, and l2_l1.

Week 3: Support vector machines

This week I went through the process of how SVM works and the three different types of kernels in SVM: linear, RBF, and polynomial. I understood the difference between SVM and logistic regression. I applied SVM to riding mowers and the cancer treatment data set. I evaluated the performance of each model. I created a web interface for my model so that when we enter the input, we can get whether the owner will get the owner or not.

Week 4: Decision Trees:

I understood the concepts of decision trees, variance, entropy, the Gini index, and hyperparameter tuning techniques like grid search CV and random search CV. I applied decision trees to the universal bank data set and evaluated the performance of my model.

Week 5: Ensemble Techniques:

I got introduced to various assembly techniques like bagging and boosting. Random forest, Ada boost, and XGboost techniques I applied these techniques to a data set and observed the performance of each model.

Week 6: Text Mining This is an interesting topic that I enjoyed. I was introduced to lemmatization and tokenization techniques. IDF, POS tagging, etc. I applied the technique called single value decomposition, refined the technique called early stopping, and evaluated the model's performance.

Week 7: Neural networks:

I was introduced to the basic concepts of neural networks and became familiar with perceptrons and simple ANN. I applied the concept of using an MLP classifier to a handwritten digit data set and evaluated the model's performance.

Week 8: Deep neural networks:

I was introduced to the concept of deep neural networks. In this, I learned about the Keras library and Tensorflow framework, applied them to a handwritten data set, and observed the performance of the model on my data set.

Week 9: CNN (Convolutional Neural Networks): I understood the concept of CNN and applied it to the images that contained different apples. I evaluated the performance of the model.

Week 10: RNN (recurrent neural networks): I understood the concept of time series and sequential analysis. I have taken the PAPA JOHNS stock data for 100 days, used RNN models LSTM and Conv 1d by storing the sequences for 9 days, predicted the 10th-day stock price, and evaluated how different RNN models perform.

Week 11: Autoencoders I understood the concept of autoencoders and applied it to the MNIST data set. I also created new images of 5 with the first letter of my name, trained the model well, and reconstructed the MNIST data set digits and my new images.

Note: Each notebook contains the markup and also the analysis of references that I observed. The folder name is easily understandable. Walk through the notebooks for further insights.
